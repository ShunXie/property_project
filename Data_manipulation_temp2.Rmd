---
title: "Data manipulation"
author: "ShunXie"
date: "2022-12-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Manipulation

```{r}
library(tidyverse)

library(dplyr)

```


## Data 1

from website https://www.census.gov/quickfacts/fact/table/bronxcountynewyork/HSG495221#HSG495221


Read all datas
```{r}
All_files_names = list.files("data/")

All_files = 
  read_csv("initialdata/propertyval.csv", show_col_types = FALSE) %>% 
  janitor::clean_names() %>% 
  select(-fact_note)

for (i in 1:length(All_files_names)){
  newfile = read_csv(str_c("data/",All_files_names[i]), show_col_types = FALSE) %>% 
    janitor::clean_names() %>% 
    select(-fact,-fact_note)
  
  All_files = cbind(All_files, newfile)
}

Cleaned_file = 
  All_files %>% 
  janitor::clean_names() %>% 
  select(-starts_with("value"))

colnames(Cleaned_file)<-gsub("_new_york","",colnames(Cleaned_file))

```

```{r}
m1 <- t(Cleaned_file)
Merged_file_temp <- data.frame(r1= row.names(m1), m1, row.names=NULL) 

names(Merged_file_temp) <- Merged_file_temp[1,] 
Merged_file <- Merged_file_temp[-1,] 
Merged_file <- Merged_file[,1:68] 
Merged_file = Merged_file %>% 
  rename(county_name=fact)

write.csv(Merged_file, "result/Merged_data.csv", row.names=FALSE)
```

```{r}
library("xlsx")
# Write the first data set in a new workbook
write.xlsx(Merged_file, file = "result/Merged_file.xlsx",
      sheetName = "Merged_data", append = FALSE)
```


## Data 2

from website https://www.nar.realtor/research-and-statistics/housing-statistics/county-median-home-prices-and-monthly-mortgage-payment



```{r}
#require(tidyverse)
#require(pdftools)
#require(lubridate)
#require(kableExtra)
library(devtools)
#devtools::install_github("ropensci/tabulizer")
library(tabulizer)
```

```{r}
rawfile <- "datawithallcounty/2022Q3.pdf"
rawlines <- extract_tables(rawfile)

```




```{r}
dsQ3 = rawlines[[1]]

for (i in 2:(length(rawlines)-1)){
dsQ3 = rbind(dsQ3, rawlines[[i]])
  }

```

```{r}
dsQ3_new = janitor::row_to_names(dsQ3, row_number = 1) 
dsQ3_clean = dsQ3_new %>%
  as.tibble() %>% 
  janitor::clean_names() %>% 
  separate(county_name, into = c("county_name", "state_name"), sep = ",") %>% 
  mutate(median_2022=as.numeric(gsub(pattern = "[^0-9.-]", replacement = '',median_home_price_q3_2022)),
         monthly_pay_2021=as.numeric(gsub(pattern = "[^0-9.-]", replacement = '',monthly_payment_q3_2021)),
         monthly_pay_2022=as.numeric(gsub(pattern = "[^0-9.-]", replacement = '',monthly_payment_q3_2022)),
         county_name = janitor::make_clean_names(county_name)) %>% 
  select(-median_home_price_q3_2022,-monthly_payment_q3_2021,-monthly_payment_q3_2022,-state_name) 
  
dsQ3_clean

```




```{r}
all_data = inner_join(Merged_file,dsQ3_clean, by="county_name") 
```




## Data3

```{r}
#install.packages("rvest")
library(rvest)

url<- read_html("https://en.wikipedia.org/wiki/List_of_school_districts_in_New_York")

Schooling <- url %>% html_table(fill=TRUE)

school_df <- Schooling[[1]]
#html_nodes("td , .headerSortUp") %>% 



clean_school_temp = 
  school_df %>%
  janitor::clean_names() %>% 
  mutate("county_name" = gsub(patter=" ", replacement = "_", tolower(paste0(county, '_county')) ),
         "student" = as.numeric(gsub(pattern = "[^0-9.-]", replacement = '',students_2))) %>% 
  select(-county,-students_2)
  #filter(student>0)
  
stat_school = 
  clean_school_temp %>% 
  group_by(county_name) %>% 
  summarize(num_school_district=n(),
            student_n = sum(student))

```

Merge together

```{r}
all_data = left_join(all_data,stat_school, by="county_name") 

```


### Clean all data and Calculate rate of increase


```{r}
library(janitor)
all_data_clean = all_data %>% clean_names() %>% mutate(median_value_of_owner_occupied_housing_units_2017_2021=as.numeric(gsub(pattern = "[^0-9.-]", replacement = '',median_value_of_owner_occupied_housing_units_2017_2021)))
#change in mortgage payment
all_data_clean$percent_increase_mortgage = round((all_data$monthly_pay_2022-all_data$monthly_pay_2021)/all_data$monthly_pay_2021*100,2)
#change in price
all_data_clean$percent_increase_house_price = round((all_data$median_2022-all_data_clean$median_value_of_owner_occupied_housing_units_2017_2021)/all_data_clean$median_value_of_owner_occupied_housing_units_2017_2021*100,2)
  

```



# Data Analysis

## Correlation

```{r}
all_data_num = 
  all_data_clean %>% 
  mutate(
    black_or_african_american_alone_percent =  as.numeric(gsub(pattern = "[^0-9.-]", replacement = '',black_or_african_american_alone_percent)),
    population_percent_change_april_1_2020_estimates_base_to_july_1_2021_v2021=  as.numeric(gsub(pattern = "[^0-9.-]", replacement = '',population_percent_change_april_1_2020_estimates_base_to_july_1_2021_v2021)),
    language_other_than_english_spoken_at_home_percent_of_persons_age_5_years_2017_2021=  as.numeric(gsub(pattern = "[^0-9.-]", replacement = '',language_other_than_english_spoken_at_home_percent_of_persons_age_5_years_2017_2021)),
    bachelors_degree_or_higher_percent_of_persons_age_25_years_2017_2021=  as.numeric(gsub(pattern = "[^0-9.-]", replacement = '',bachelors_degree_or_higher_percent_of_persons_age_25_years_2017_2021)),
    total_health_care_and_social_assistance_receipts_revenue_2017_1_000=  as.numeric(gsub(pattern = "[^0-9.-]", replacement = '',total_health_care_and_social_assistance_receipts_revenue_2017_1_000)),
    mean_travel_time_to_work_minutes_workers_age_16_years_2017_2021=  as.numeric(gsub(pattern = "[^0-9.-]", replacement = '',mean_travel_time_to_work_minutes_workers_age_16_years_2017_2021)),
    median_household_income_in_2021_dollars_2017_2021=  as.numeric(gsub(pattern = "[^0-9.-]", replacement = '',median_household_income_in_2021_dollars_2017_2021)),
    total_employment_percent_change_2019_2020=  as.numeric(gsub(pattern = "[^0-9.-]", replacement = '',total_employment_percent_change_2019_2020)),
    population_per_square_mile_2020=  as.numeric(gsub(pattern = "[^0-9.-]", replacement = '',population_per_square_mile_2020))) %>% 
      mutate(
        median_house = median_value_of_owner_occupied_housing_units_2017_2021,
        black=black_or_african_american_alone_percent,
        pop_change = population_percent_change_april_1_2020_estimates_base_to_july_1_2021_v2021, 
        non_english=language_other_than_english_spoken_at_home_percent_of_persons_age_5_years_2017_2021, 
        bachelor=bachelors_degree_or_higher_percent_of_persons_age_25_years_2017_2021,
        healthcare = total_health_care_and_social_assistance_receipts_revenue_2017_1_000,
        travel_time=mean_travel_time_to_work_minutes_workers_age_16_years_2017_2021,
        median_income=median_household_income_in_2021_dollars_2017_2021,
        change_employment=total_employment_percent_change_2019_2020,
        pop_per_mile = population_per_square_mile_2020
        )
        
```


```{r}
library(corrplot)
library(cowplot)
House_data = all_data_num %>% 
  select(median_house, black, pop_change, non_english,bachelor,healthcare,travel_time,median_income,change_employment,pop_per_mile) %>% 
  na.omit()

House_data_all = all_data_num %>% 
  select(median_house, median_2022, monthly_pay_2021,monthly_pay_2022,percent_increase_mortgage, percent_increase_house_price,black, pop_change, non_english,bachelor,healthcare,travel_time,median_income,change_employment,pop_per_mile) %>% 
  na.omit()


res = cor(House_data)
corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```





## lm model

```{r}
  
lmmodel = lm(median_value_of_owner_occupied_housing_units_2017_2021~black_or_african_american_alone_percent+population_percent_change_april_1_2020_estimates_base_to_july_1_2021_v2021+language_other_than_english_spoken_at_home_percent_of_persons_age_5_years_2017_2021+bachelors_degree_or_higher_percent_of_persons_age_25_years_2017_2021+total_health_care_and_social_assistance_receipts_revenue_2017_1_000+mean_travel_time_to_work_minutes_workers_age_16_years_2017_2021+median_household_income_in_2021_dollars_2017_2021+total_employment_percent_change_2019_2020+population_per_square_mile_2020,all_data_num)


```

```{r}
summary(lmmodel)
```

```{r}
lmmodel2 = lm(median_2022~black_or_african_american_alone_percent+population_percent_change_april_1_2020_estimates_base_to_july_1_2021_v2021+language_other_than_english_spoken_at_home_percent_of_persons_age_5_years_2017_2021+bachelors_degree_or_higher_percent_of_persons_age_25_years_2017_2021+total_health_care_and_social_assistance_receipts_revenue_2017_1_000+mean_travel_time_to_work_minutes_workers_age_16_years_2017_2021+median_household_income_in_2021_dollars_2017_2021+total_employment_percent_change_2019_2020+population_per_square_mile_2020,all_data_num)

summary(lmmodel2)
```


```{r}
lmmodel3 = lm(monthly_pay_2021~black_or_african_american_alone_percent+population_percent_change_april_1_2020_estimates_base_to_july_1_2021_v2021+language_other_than_english_spoken_at_home_percent_of_persons_age_5_years_2017_2021+bachelors_degree_or_higher_percent_of_persons_age_25_years_2017_2021+total_health_care_and_social_assistance_receipts_revenue_2017_1_000+mean_travel_time_to_work_minutes_workers_age_16_years_2017_2021+median_household_income_in_2021_dollars_2017_2021+total_employment_percent_change_2019_2020+population_per_square_mile_2020,all_data_num)

summary(lmmodel3)
```


```{r}
lmmodel4 = lm(percent_increase_mortgage~black_or_african_american_alone_percent+population_percent_change_april_1_2020_estimates_base_to_july_1_2021_v2021+language_other_than_english_spoken_at_home_percent_of_persons_age_5_years_2017_2021+bachelors_degree_or_higher_percent_of_persons_age_25_years_2017_2021+total_health_care_and_social_assistance_receipts_revenue_2017_1_000+mean_travel_time_to_work_minutes_workers_age_16_years_2017_2021+median_household_income_in_2021_dollars_2017_2021+total_employment_percent_change_2019_2020+population_per_square_mile_2020,all_data_num)

summary(lmmodel4)
```



## Lasso Selection

```{r}
# supply sequence of lambda values for the lasso cross validation for lambda
lambda_seq <- 10^seq(-3, 0, by = .1)
set.seed(2022)
# save matrix of predictors to pass to the lasso function
Data_X <-
House_data %>% 
select(-median_house) %>% 
as.matrix()
Data_y <-
House_data_all %>% 
select(percent_increase_house_price) %>% 
as.matrix()
cv_lasso_fit <- glmnet::cv.glmnet(x = Data_X,
y = Data_y,
lambda = lambda_seq,
nfolds = 5)
cv_lasso_fit


lasso_fit <- glmnet::glmnet(x = Data_X,
y = Data_y,
lambda = cv_lasso_fit$lambda.min)
coef(lasso_fit)
```

```{r}
#adjusted R square
library(grid)
library(gridExtra)
library(cowplot)
library(corrplot)
library(leaps)
library(glmnet)
library(caret)
# Printing the 2 best models of each size, using the adjusted R square criterion:
leaps(x = Data_X, y = Data_y, nbest = 2, method = "adjr2")

```
```{r}


best = regsubsets(percent_increase_house_price ~ ., data = House_data_all)
rs = summary(best)
# plot of Cp and Adj-R2 as functions of parameters
par(mfrow=c(1,2))
plot(2:9, rs$cp, xlab="No of parameters", ylab="Cp Statistic")
abline(0,1)
plot(2:9, rs$adjr2, xlab="No of parameters", ylab="Adj R2")
```


## Pearson Correlation and Distance Correlation

 A small difference between the Pearson and distance correlation values might associate with linearity. A greater Pearson coefficient than distance correlation value indicates the potential for outliers in the direction of the regression line of the normal data. A smaller Pearson coefficient than the distance correlation value indicates a nonlinear relationship or outliers in the normal direction of the regression line.

```{r}

House_data = all_data_num %>% 
  dplyr::select(median_house, black, pop_change, non_english,bachelor,healthcare,travel_time,median_income,change_employment,pop_per_mile,num_school_district,student_n) %>% 
  na.omit()

res = cor(House_data)

library(ppcor)

res1 = pcor(House_data)

corrplot(res1$estimate, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)



#library(dccpp)
library(dcov)






Pearson_table = tibble(var_name=colnames(data.frame(res1$estimate)), Pearson_corr =data.frame(res)$median_house, Partial_Pearson_corr=data.frame(res1$estimate)$median_house
)
Pearson_table=Pearson_table[2:12,]

Pearson_table$dcor = 0
Pearson_table$pdcor = 0

House_data_temp = House_data[,-1]
for (i in 1:11){
  Pearson_table$dcor[i] = dcor(House_data[,i+1], House_data$median_house)
  Pearson_table$pdcor[i] = pdcor(House_data[,i+1], House_data$median_house,House_data_temp[,-i])

}






Pearson_table



```


```{r}
House_data2 = all_data_num %>% 
  dplyr::select(median_2022, black, pop_change, non_english,bachelor,healthcare,travel_time,median_income,change_employment,pop_per_mile,num_school_district,student_n) %>% 
  na.omit()


res2 = cor(House_data2)
corrplot(res2, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)

res2_p = pcor(House_data2)
corrplot(res2_p$estimate, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)




Pearson_table2 = tibble(var_name=colnames(data.frame(res2_p$estimate)), Pearson_corr =data.frame(res2)$median_2022, Partial_Pearson_corr=data.frame(res2_p$estimate)$median_2022
)

Pearson_table2=Pearson_table2[2:12,]

Pearson_table2$dcor = 0
Pearson_table2$pdcor = 0

House_data_temp2 = House_data2[,-1]
for (i in 1:11){
  Pearson_table2$dcor[i] = dcor(House_data2[,i+1], House_data2$median_2022)
  Pearson_table2$pdcor[i] = pdcor(House_data2[,i+1], House_data2$median_2022,House_data_temp2[,-i])

}


Pearson_table2

```





